{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Core Python libraries\n",
        "!pip install numpy pandas\n",
        "\n",
        "# PDF and document processing\n",
        "!pip install pymupdf  # PyMuPDF for PDF processing\n",
        "\n",
        "# OCR libraries\n",
        "!pip install pytesseract pillow opencv-python\n",
        "\n",
        "# Vector database and embeddings\n",
        "!pip install chromadb sentence-transformers\n",
        "\n",
        "# LLM integration\n",
        "!pip install openai\n",
        "\n",
        "# Additional utilities\n",
        "!pip install python-dotenv  # For environment variable management"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFN3XhHFjGPz",
        "outputId": "c342237d-3baa-4f19-ecde-9f8477e2f9e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.23.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.31.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m837.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=b23f2ef865928c1f45a35e823b15e6d79bd7af1bc8f7a006281539a76098bbf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, nvidia-cusolver-cu12, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.0 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.21.0 opentelemetry-api-1.31.1 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-sdk-1.31.1 opentelemetry-semantic-conventions-0.52b1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.23.0 protobuf-5.29.4 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 starlette-0.45.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf pytesseract pillow opencv-python-headless sentence-transformers chromadb --quiet\n",
        "!apt-get install -y tesseract-ocr\n"
      ],
      "metadata": {
        "id": "OSgC0mPVmIKH",
        "outputId": "7825dbea-f9f9-4c28-f737-8500692591b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,901 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai --quiet\n",
        "\n"
      ],
      "metadata": {
        "id": "nKoLDAo_lekv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=os.getenv(\"AIzaSyAWe5EmcArFK21WosWTnRGef3hPsh8RbkU\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5QsiM4G9mF-n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtaOWGOz9YYl",
        "outputId": "b0d5a5cb-ff3e-4657-dff6-300c264de398"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "import chromadb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkP_rrQp-YXp",
        "outputId": "e601e628-5ea7-422d-90b3-ff762d09045f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.23.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf pytesseract opencv-python pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EEkqakA-8q1",
        "outputId": "2fe3a754-9370-44b0-cb5f-de9f1e7fe327"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF for PDF reading\n",
        "from PIL import Image  # For image processing\n",
        "import cv2  # OpenCV for image preprocessing\n",
        "import io  # For in-memory byte streams\n",
        "import pytesseract  # OCR (Optical Character Recognition)\n",
        "from datetime import datetime  # For timestamps\n",
        "import json  # To handle JSON formatting and dumping\n",
        "import regex as re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NjEmPWEb-_7S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "I5neT__Ci4SK"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pytesseract\n",
        "from datetime import datetime\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import google.generativeai as genai\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "LLM_MODEL = \"gemini-1.5-flash-001\"\n",
        "\n",
        "# ======================== SUPPLY CHAIN RAG CLASS ========================\n",
        "class SupplyChainRAGSystem:\n",
        "    def __init__(self, collection_name=\"supply_chain_docs\"):\n",
        "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        self.reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "        self.chroma_client = chromadb.Client()\n",
        "        try:\n",
        "            self.collection = self.chroma_client.get_collection(collection_name)\n",
        "        except:\n",
        "            self.collection = self.chroma_client.create_collection(collection_name)\n",
        "\n",
        "    def process_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extracts page number and ORIGINAL text from PDF using 'blocks' mode.\"\"\"\n",
        "        document = fitz.open(pdf_path)\n",
        "        extracted_pages = []\n",
        "\n",
        "        for page_num, page in enumerate(document):\n",
        "            # --- CHANGE THE EXTRACTION MODE HERE ---\n",
        "            # Get text as blocks, hoping to preserve lines better\n",
        "            blocks = page.get_text(\"blocks\")\n",
        "            # --- Combine text from blocks on the page ---\n",
        "            original_text = \"\"\n",
        "            for b in blocks:\n",
        "                original_text += b[4] # The 5th element is the text content of the block\n",
        "\n",
        "            # --- Optional: Add image OCR text (keep as before) ---\n",
        "            if len(original_text.strip()) < 100: # Check combined block text length\n",
        "                print(f\"Page {page_num}: Text length low, attempting OCR...\")\n",
        "                try:\n",
        "                    image_list = page.get_images(full=True)\n",
        "                    ocr_add_text = \"\"\n",
        "                    if image_list:\n",
        "                        print(f\"Page {page_num}: Found {len(image_list)} image(s) for OCR.\")\n",
        "                        for img_index, img_info in enumerate(image_list):\n",
        "                            xref = img_info[0]\n",
        "                            base_image = document.extract_image(xref)\n",
        "                            if not base_image:\n",
        "                                print(f\"Page {page_num}, Img {img_index}: Could not extract base image.\")\n",
        "                                continue\n",
        "                            image_bytes = base_image[\"image\"]\n",
        "                            if not image_bytes:\n",
        "                                print(f\"Page {page_num}, Img {img_index}: Extracted image has no bytes.\")\n",
        "                                continue\n",
        "\n",
        "                            # --- IMAGE PROCESSING AND THRESH DEFINITION (Inside the loop) ---\n",
        "                            try:\n",
        "                                image = Image.open(io.BytesIO(image_bytes))\n",
        "                                # Convert to OpenCV format (ensure it's BGR if possible, handle different modes)\n",
        "                                if image.mode == 'RGBA':\n",
        "                                    image_np = cv2.cvtColor(np.array(image), cv2.COLOR_RGBA2BGR)\n",
        "                                elif image.mode == 'RGB':\n",
        "                                    image_np = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "                                elif image.mode == 'L': # Grayscale\n",
        "                                    image_np = cv2.cvtColor(np.array(image), cv2.COLOR_GRAY2BGR)\n",
        "                                else: # Other modes might need specific handling\n",
        "                                    print(f\"Page {page_num}, Img {img_index}: Skipping image with unhandled mode {image.mode}\")\n",
        "                                    continue\n",
        "\n",
        "                                gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n",
        "                                # Apply thresholding - this defines 'thresh'\n",
        "                                thresh_val, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "                                print(f\"Page {page_num}, Img {img_index}: Processed image, threshold value: {thresh_val}\")\n",
        "\n",
        "                                # Now call pytesseract with the defined 'thresh'\n",
        "                                ocr_text_from_image = pytesseract.image_to_string(thresh)\n",
        "                                if ocr_text_from_image.strip():\n",
        "                                    print(f\"Page {page_num}, Img {img_index}: OCR successful.\")\n",
        "                                    ocr_add_text += \"\\n\" + ocr_text_from_image\n",
        "                                else:\n",
        "                                    print(f\"Page {page_num}, Img {img_index}: OCR returned empty text.\")\n",
        "\n",
        "                            except Exception as img_proc_err:\n",
        "                                print(f\"Page {page_num}, Img {img_index}: Error processing image for OCR: {img_proc_err}\")\n",
        "                            # --- END IMAGE PROCESSING ---\n",
        "\n",
        "                        # Append combined OCR text (if any) for the page\n",
        "                        if ocr_add_text:\n",
        "                            original_text += \"\\n=== OCR Text ===\\n\" + ocr_add_text # Add separator\n",
        "                    else:\n",
        "                        print(f\"Page {page_num}: No images found for OCR despite low text.\")\n",
        "\n",
        "                except Exception as ocr_err:\n",
        "                    print(f\"Warning: OCR failed on page {page_num}: {ocr_err}\")\n",
        "            # --- End Optional OCR ---\n",
        "\n",
        "            if original_text.strip():\n",
        "                extracted_pages.append({\n",
        "                    \"page_num\": page_num,\n",
        "                    \"original_text\": original_text, # Store the text with (hopefully better) structure\n",
        "                    \"source\": pdf_path,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Warning: No text extracted from page {page_num} using 'blocks' mode.\")\n",
        "\n",
        "        document.close()\n",
        "        print(f\"process_pdf extracted text from {len(extracted_pages)} pages using 'blocks' mode.\")\n",
        "        return extracted_pages\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        text = text.replace('|', 'I').replace('0', 'O')\n",
        "        return text\n",
        "\n",
        "    def chunk_document(self, extracted_pages: List[Dict[str, Any]], chunk_size: int = 1000) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Chunks the CLEANED text for RAG indexing.\"\"\"\n",
        "        chunks = []\n",
        "        for page_data in extracted_pages:\n",
        "            # Apply cleaning HERE before chunking\n",
        "            cleaned_text = self._clean_text(page_data[\"original_text\"])\n",
        "\n",
        "            if not cleaned_text.strip(): # Skip if page became empty after cleaning\n",
        "                continue\n",
        "\n",
        "            for i in range(0, len(cleaned_text), chunk_size):\n",
        "                chunk_text = cleaned_text[i:i + chunk_size]\n",
        "                if chunk_text.strip():\n",
        "                    chunks.append({\n",
        "                        \"text\": chunk_text, # Store cleaned, chunked text\n",
        "                        \"page_num\": page_data[\"page_num\"],\n",
        "                        \"source\": page_data[\"source\"],\n",
        "                        \"chunk_id\": f\"{page_data['source']}_p{page_data['page_num']}_{i // chunk_size}\"\n",
        "                    })\n",
        "        return chunks\n",
        "\n",
        "    def index_chunks(self, chunks: List[Dict[str, Any]]) -> None:\n",
        "        ids = []\n",
        "        documents = []\n",
        "        metadatas = []\n",
        "\n",
        "        for chunk in chunks:\n",
        "            ids.append(chunk[\"chunk_id\"])\n",
        "            documents.append(chunk[\"text\"])\n",
        "            metadatas.append({\n",
        "                \"source\": chunk[\"source\"],\n",
        "                \"page_num\": chunk[\"page_num\"]\n",
        "            })\n",
        "\n",
        "        embeddings = self.embedding_model.encode(documents).tolist()\n",
        "\n",
        "        self.collection.add(\n",
        "            ids=ids,\n",
        "            embeddings=embeddings,\n",
        "            documents=documents,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "\n",
        "    def generate_hyde_query(self, query: str) -> str:\n",
        "        prompt = f\"\"\"Based on the following question about supply chain data,\n",
        "generate a hypothetical document snippet that would contain the answer:\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Generate a realistic supply chain document snippet:\"\"\"\n",
        "\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    def detect_anomalies(self, query: str, top_k: int = 5) -> Dict[str, Any]:\n",
        "        hyde_doc = self.generate_hyde_query(query)\n",
        "        hyde_embedding = self.embedding_model.encode([hyde_doc])[0].tolist()\n",
        "\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[hyde_embedding],\n",
        "            n_results=top_k * 2\n",
        "        )\n",
        "\n",
        "        retrieved_texts = results['documents'][0]\n",
        "        rerank_pairs = [[query, doc] for doc in retrieved_texts]\n",
        "        rerank_scores = self.reranker.predict(rerank_pairs)\n",
        "        reranked_indices = np.argsort(rerank_scores)[::-1][:top_k]\n",
        "\n",
        "        top_chunks = [retrieved_texts[i] for i in reranked_indices]\n",
        "        top_metadatas = [results['metadatas'][0][i] for i in reranked_indices]\n",
        "\n",
        "        anomaly_prompt = f\"\"\"Analyze the following supply chain information for potential anomalies or issues.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Retrieved Information:\n",
        "{json.dumps(top_chunks, indent=2)}\n",
        "\n",
        "Based on this information, identify any:\n",
        "1. Delivery delays or timeline inconsistencies\n",
        "2. Price fluctuations outside normal ranges\n",
        "3. Quantity discrepancies\n",
        "4. Quality issues\n",
        "5. Supplier performance problems\n",
        "6. Logistical bottlenecks\n",
        "7. Carbon Footprint\n",
        "\n",
        "Provide a detailed analysis with specific anomalies found:\"\"\"\n",
        "\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "        response = model.generate_content(anomaly_prompt)\n",
        "        analysis = response.text\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"hyde_document\": hyde_doc,\n",
        "            \"top_results\": [{\"text\": t, \"metadata\": m} for t, m in zip(top_chunks, top_metadatas)],\n",
        "            \"anomaly_analysis\": analysis\n",
        "        }\n",
        "\n",
        "    # ======================== UPDATED DATA EXTRACTION FUNCTION ========================\n",
        "    def extract_structured_data(self, text_chunks: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract structured data from text chunks with improved parsing for supplier quality\n",
        "        \"\"\"\n",
        "        # First try direct parsing for supplier quality data\n",
        "        supplier_quality_data = []\n",
        "\n",
        "        for chunk in text_chunks:\n",
        "            if \"SUPPLIER QUALITY ASSESSMENT\" in chunk:\n",
        "                # Extract the table section\n",
        "                table_section = chunk.split(\"SUPPLIER QUALITY ASSESSMENT\")[1]\n",
        "                if \"CARBON FOOTPRINT ANALYSIS\" in table_section:\n",
        "                    table_section = table_section.split(\"CARBON FOOTPRINT ANALYSIS\")[0]\n",
        "\n",
        "                lines = table_section.strip().split(\"\\n\")\n",
        "                if len(lines) < 2:  # Need at least header and one data row\n",
        "                    continue\n",
        "\n",
        "                # Process each line of the table\n",
        "                for line in lines[1:]:  # Skip header line\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) < 5:  # Not enough data\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        # Handle supplier name (might be multiple words)\n",
        "                        supplier_name_parts = []\n",
        "                        product_started = False\n",
        "                        product_parts = []\n",
        "                        quality_score = None\n",
        "\n",
        "                        for i, part in enumerate(parts):\n",
        "                            # Try to detect when product column starts\n",
        "                            if not product_started and part in [\"Electronic\", \"Raw\", \"Component\"]:\n",
        "                                product_started = True\n",
        "                                product_parts.append(part)\n",
        "                            # Try to detect quality score (should be a number)\n",
        "                            elif product_started and not quality_score and part.replace('.', '', 1).isdigit():\n",
        "                                quality_score = float(part)\n",
        "                                break\n",
        "                            elif product_started:\n",
        "                                product_parts.append(part)\n",
        "                            else:\n",
        "                                supplier_name_parts.append(part)\n",
        "\n",
        "                        # If we couldn't parse properly, skip\n",
        "                        if not supplier_name_parts or not product_parts or not quality_score:\n",
        "                            continue\n",
        "\n",
        "                        supplier_quality_data.append({\n",
        "                            \"supplier\": ' '.join(supplier_name_parts),\n",
        "                            \"product\": ' '.join(product_parts),\n",
        "                            \"quality_score\": quality_score\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error parsing line: {line}. Error: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "        # If direct parsing found data, use it\n",
        "        if supplier_quality_data:\n",
        "            print(f\"Found {len(supplier_quality_data)} supplier quality records through direct parsing\")\n",
        "            return pd.DataFrame(supplier_quality_data)\n",
        "\n",
        "        # Fallback to LLM extraction\n",
        "        prompt = f\"\"\"Extract structured supply chain data from the following text.\n",
        "        Focus specifically on supplier quality metrics from the SUPPLIER QUALITY ASSESSMENT section.\n",
        "\n",
        "        Text chunks:\n",
        "        {json.dumps(text_chunks, indent=2)}\n",
        "\n",
        "        Return ONLY a JSON array of objects with these exact keys:\n",
        "        \"supplier\" (company name), \"product\" (product name/type), \"quality_score\" (numerical score)\n",
        "\n",
        "        For missing values, use null. Format dates as YYYY-MM-DD.\"\"\"\n",
        "\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        try:\n",
        "            data = json.loads(response.text)\n",
        "            if data and \"supplier\" in data[0] and \"quality_score\" in data[0]:\n",
        "                return pd.DataFrame(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in LLM JSON parsing: {str(e)}\")\n",
        "\n",
        "        # Final fallback - create a minimal dataset based on the PDF content\n",
        "        print(\"Using hardcoded supplier quality data as fallback\")\n",
        "        return pd.DataFrame({\n",
        "            \"supplier\": [\"TechSupply\", \"PrimeParts\", \"GlobalComp\", \"IndustrialX\",\n",
        "                         \"MetalWorks\", \"ComponentsY\", \"FastSupply\", \"PrecisionZ\"],\n",
        "            \"product\": [\"Electronic A\", \"Electronic B\", \"Electronic C\", \"Raw Material D\",\n",
        "                       \"Raw Material E\", \"Component F\", \"Component G\", \"Electronic H\"],\n",
        "            \"quality_score\": [92, 88, 79, 90, 82, 95, 76, 91]\n",
        "        })\n",
        "\n",
        "    # ======================== UPDATED VISUALIZATION SUGGESTION FUNCTION ========================\n",
        "    def suggest_visualizations(self, data: pd.DataFrame, analysis: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Use LLM to suggest appropriate visualizations with improved supplier quality handling\n",
        "        \"\"\"\n",
        "        # Check if we have supplier quality data\n",
        "        has_supplier_data = 'supplier' in data.columns and 'quality_score' in data.columns\n",
        "\n",
        "        # Ensure we include supplier quality visualization if data is available\n",
        "        if has_supplier_data:\n",
        "            default_suggestions = [\n",
        "                {\n",
        "                    \"type\": \"bar_chart\",\n",
        "                    \"columns\": [\"supplier\", \"quality_score\"],\n",
        "                    \"purpose\": \"Compare supplier quality\",\n",
        "                    \"anomaly_threshold\": {\"lower\": 85}  # Based on the 85% threshold mentioned in the document\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            # Try to get other suggestions from LLM\n",
        "            try:\n",
        "                data_sample = data.head(5).to_dict(orient='records')\n",
        "                prompt = f\"\"\"You are a supply chain visualization expert. Based on the following data sample and analysis,\n",
        "                suggest 2-4 additional visualizations (not including supplier quality comparison) that would highlight anomalies and issues.\n",
        "\n",
        "                Data Sample:\n",
        "                {json.dumps(data_sample, indent=2)}\n",
        "\n",
        "                Analysis of Issues:\n",
        "                {analysis}\n",
        "\n",
        "                For each visualization, provide:\n",
        "                1. The type of chart (time_series, heatmap, network, scatter, etc.)\n",
        "                2. The columns to use\n",
        "                3. The purpose of the visualization\n",
        "                4. Any anomaly thresholds to highlight\n",
        "\n",
        "                Return ONLY a JSON array of visualization objects.\"\"\"\n",
        "\n",
        "                model = genai.GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "                response = model.generate_content(prompt)\n",
        "\n",
        "                try:\n",
        "                    additional_suggestions = json.loads(response.text)\n",
        "                    return default_suggestions + additional_suggestions\n",
        "                except:\n",
        "                    # If parsing fails, just add a few standard visualizations\n",
        "                    if 'date' in data.columns and 'delivery_time' in data.columns:\n",
        "                        default_suggestions.append({\n",
        "                            \"type\": \"time_series\",\n",
        "                            \"columns\": [\"date\", \"delivery_time\"],\n",
        "                            \"purpose\": \"Track delivery time trends\",\n",
        "                            \"anomaly_threshold\": None\n",
        "                        })\n",
        "\n",
        "                    return default_suggestions\n",
        "            except:\n",
        "                return default_suggestions\n",
        "\n",
        "        # If no supplier data, fall back to original method\n",
        "        data_sample = data.head(5).to_dict(orient='records')\n",
        "        prompt = f\"\"\"You are a supply chain visualization expert. Based on the following data sample and analysis,\n",
        "        suggest 3-5 visualizations that would best highlight the anomalies and issues in the supply chain.\n",
        "\n",
        "        Data Sample:\n",
        "        {json.dumps(data_sample, indent=2)}\n",
        "\n",
        "        Analysis of Issues:\n",
        "        {analysis}\n",
        "\n",
        "        For each visualization, provide:\n",
        "        1. The type of chart (time_series, bar_chart, heatmap, network, scatter, etc.)\n",
        "        2. The columns to use\n",
        "        3. The purpose of the visualization\n",
        "        4. Any anomaly thresholds to highlight\n",
        "\n",
        "        Return ONLY a JSON array of visualization objects.\"\"\"\n",
        "\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        try:\n",
        "            vis_suggestions = json.loads(response.text)\n",
        "            return vis_suggestions\n",
        "        except:\n",
        "            # Fallback suggestions if parsing fails\n",
        "            return [\n",
        "                {\n",
        "                    \"type\": \"time_series\",\n",
        "                    \"columns\": [\"date\", \"delivery_time\"],\n",
        "                    \"purpose\": \"Track delivery time trends\",\n",
        "                    \"anomaly_threshold\": None\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"bar_chart\",\n",
        "                    \"columns\": [\"supplier\", \"quality_score\"],\n",
        "                    \"purpose\": \"Compare supplier quality\",\n",
        "                    \"anomaly_threshold\": {\"lower\": 85}\n",
        "                }\n",
        "            ]\n",
        "\n",
        "    # ======================== IMPROVED BAR CHART FUNCTION ========================\n",
        "    def generate_bar_chart(self, df: pd.DataFrame, x_col: str, y_col: str, title: str,\n",
        "                          anomaly_threshold: float = None) -> plt.Figure:\n",
        "        \"\"\"Generate bar chart with improved data checking and debugging\"\"\"\n",
        "        # Debug information\n",
        "        print(f\"Generating bar chart for {x_col} vs {y_col}\")\n",
        "        print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
        "        print(f\"DataFrame shape: {df.shape}\")\n",
        "        print(f\"First few rows:\\n{df.head()}\")\n",
        "\n",
        "        if x_col not in df.columns or y_col not in df.columns:\n",
        "            print(f\"Missing columns: {x_col} or {y_col} not in dataframe\")\n",
        "            # Create empty plot with message\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            ax.text(0.5, 0.5, f\"Insufficient data for bar chart\\nMissing columns: {x_col if x_col not in df.columns else ''} {y_col if y_col not in df.columns else ''}\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"DataFrame is empty\")\n",
        "            # Create empty plot with message\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            ax.text(0.5, 0.5, \"Insufficient data for bar chart\\nEmpty dataset\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "        # Ensure data is usable for plotting\n",
        "        try:\n",
        "            if df[y_col].dtype not in ['int64', 'float64']:\n",
        "                print(f\"Converting {y_col} to numeric values\")\n",
        "                df[y_col] = pd.to_numeric(df[y_col], errors='coerce')\n",
        "\n",
        "            # Drop rows with NaN values\n",
        "            df = df.dropna(subset=[x_col, y_col])\n",
        "            if df.empty:\n",
        "                print(\"DataFrame is empty after dropping NaN values\")\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                ax.text(0.5, 0.5, \"Insufficient data for bar chart\\nNo numeric values\",\n",
        "                       ha='center', va='center', fontsize=14)\n",
        "                ax.set_title(title)\n",
        "                return fig\n",
        "\n",
        "            print(f\"Creating plot with {len(df)} rows of data\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing data: {str(e)}\")\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            ax.text(0.5, 0.5, f\"Error preparing data: {str(e)}\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        # Group by category if multiple entries\n",
        "        if df[x_col].duplicated().any():\n",
        "            data = df.groupby(x_col)[y_col].mean().sort_values(ascending=False)\n",
        "        else:\n",
        "            data = df.set_index(x_col)[y_col].sort_values(ascending=False)\n",
        "\n",
        "        # Create color map based on anomaly threshold\n",
        "        colors = ['#1f77b4'] * len(data)  # Default color\n",
        "\n",
        "        if anomaly_threshold is not None:\n",
        "            if isinstance(anomaly_threshold, dict):\n",
        "                if 'upper' in anomaly_threshold and 'lower' in anomaly_threshold:\n",
        "                    upper = anomaly_threshold.get('upper')\n",
        "                    lower = anomaly_threshold.get('lower')\n",
        "                    for i, value in enumerate(data.values):\n",
        "                        if value > upper:\n",
        "                            colors[i] = 'red'\n",
        "                        elif value < lower:\n",
        "                            colors[i] = 'orange'\n",
        "                elif 'lower' in anomaly_threshold:\n",
        "                    lower = anomaly_threshold.get('lower')\n",
        "                    for i, value in enumerate(data.values):\n",
        "                        if value < lower:\n",
        "                            colors[i] = 'orange'\n",
        "                elif 'upper' in anomaly_threshold:\n",
        "                    upper = anomaly_threshold.get('upper')\n",
        "                    for i, value in enumerate(data.values):\n",
        "                        if value > upper:\n",
        "                            colors[i] = 'red'\n",
        "            else:\n",
        "                # Use as absolute threshold\n",
        "                for i, value in enumerate(data.values):\n",
        "                    if abs(value) > anomaly_threshold:\n",
        "                        colors[i] = 'red'\n",
        "\n",
        "        # Plot\n",
        "        try:\n",
        "            data.plot(kind='bar', ax=ax, color=colors)\n",
        "            ax.set_title(title)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col)\n",
        "            ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "            # Rotate x-axis labels if needed\n",
        "            if len(data) > 5:\n",
        "                plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "            # Add threshold line if specified\n",
        "            if anomaly_threshold is not None and isinstance(anomaly_threshold, dict) and 'lower' in anomaly_threshold:\n",
        "                ax.axhline(y=anomaly_threshold['lower'], color='r', linestyle='--', alpha=0.7,\n",
        "                           label=f'Threshold ({anomaly_threshold[\"lower\"]})')\n",
        "                ax.legend()\n",
        "\n",
        "            fig.tight_layout()\n",
        "            return fig\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating plot: {str(e)}\")\n",
        "            ax.text(0.5, 0.5, f\"Error creating plot: {str(e)}\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "    def generate_time_series(self, df: pd.DataFrame, x_col: str, y_col: str, title: str,\n",
        "                             anomaly_threshold: float = None) -> plt.Figure:\n",
        "        \"\"\"Generate time series chart with anomaly detection\"\"\"\n",
        "        if x_col not in df.columns or y_col not in df.columns or df.empty:\n",
        "            # Create empty plot with message\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            ax.text(0.5, 0.5, \"Insufficient data for time series plot\",\n",
        "                    ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Ensure date column is datetime type\n",
        "        if x_col == 'date' and df[x_col].dtype != 'datetime64[ns]':\n",
        "            try:\n",
        "                df['date'] = pd.to_datetime(df['date'])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Sort by date/x-column\n",
        "        df = df.sort_values(by=x_col)\n",
        "\n",
        "        # Plot main line\n",
        "        df.plot(x=x_col, y=y_col, ax=ax, marker='o')\n",
        "\n",
        "        # Highlight anomalies if threshold provided\n",
        "        if anomaly_threshold is not None:\n",
        "            if isinstance(anomaly_threshold, dict):\n",
        "                if 'zscore' in anomaly_threshold:\n",
        "                    # Z-score based anomaly detection\n",
        "                    threshold = anomaly_threshold['zscore']\n",
        "                    y_mean = df[y_col].mean()\n",
        "                    y_std = df[y_col].std()\n",
        "                    anomalies = df[abs((df[y_col] - y_mean) / y_std) > threshold]\n",
        "                elif 'upper' in anomaly_threshold and 'lower' in anomaly_threshold:\n",
        "                    # Absolute threshold values\n",
        "                    upper = anomaly_threshold.get('upper')\n",
        "                    lower = anomaly_threshold.get('lower')\n",
        "                    anomalies = df[(df[y_col] > upper) | (df[y_col] < lower)]\n",
        "                else:\n",
        "                    anomalies = pd.DataFrame()\n",
        "            else:\n",
        "                # Use as absolute value threshold\n",
        "                anomalies = df[abs(df[y_col]) > anomaly_threshold]\n",
        "\n",
        "            if not anomalies.empty:\n",
        "                ax.scatter(anomalies[x_col], anomalies[y_col], color='red', s=80, zorder=5, label='Anomalies')\n",
        "                ax.legend()\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "        fig.tight_layout()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_heatmap(self, df: pd.DataFrame, x_col: str, y_col: str, value_col: str,\n",
        "                         title: str) -> plt.Figure:\n",
        "        \"\"\"Generate heatmap for identifying patterns and anomalies\"\"\"\n",
        "        if x_col not in df.columns or y_col not in df.columns or value_col not in df.columns or df.empty:\n",
        "            # Create empty plot with message\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "            ax.text(0.5, 0.5, \"Insufficient data for heatmap\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "        # Pivot the data\n",
        "        try:\n",
        "            pivot_data = df.pivot_table(index=y_col, columns=x_col, values=value_col, aggfunc='mean')\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(12, 8))\n",
        "            sns.heatmap(pivot_data, annot=True, cmap=\"YlGnBu\", ax=ax)\n",
        "            ax.set_title(title)\n",
        "            fig.tight_layout()\n",
        "            return fig\n",
        "        except:\n",
        "            # Fallback if pivot fails\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "            ax.text(0.5, 0.5, \"Could not generate heatmap from available data\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "    def generate_network_graph(self, df: pd.DataFrame, source_col: str, target_col: str,\n",
        "                              weight_col: str = None, title: str = \"Supply Chain Network\") -> plt.Figure:\n",
        "        \"\"\"Generate network visualization of the supply chain\"\"\"\n",
        "        if source_col not in df.columns or target_col not in df.columns or df.empty:\n",
        "            # Create empty plot with message\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "            ax.text(0.5, 0.5, \"Insufficient data for network graph\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "        # Create directed graph\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add nodes and edges\n",
        "        for _, row in df.iterrows():\n",
        "            source = row[source_col]\n",
        "            target = row[target_col]\n",
        "\n",
        "            if source not in G:\n",
        "                G.add_node(source)\n",
        "            if target not in G:\n",
        "                G.add_node(target)\n",
        "\n",
        "            if weight_col and weight_col in df.columns:\n",
        "                weight = row[weight_col]\n",
        "                G.add_edge(source, target, weight=weight)\n",
        "            else:\n",
        "                G.add_edge(source, target)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        # Calculate layout\n",
        "        try:\n",
        "            pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "            # Calculate node importance\n",
        "            centrality = nx.betweenness_centrality(G)\n",
        "            node_sizes = [centrality[node] * 2000 + 100 for node in G.nodes()]\n",
        "\n",
        "            # Draw network\n",
        "            nx.draw_networkx(\n",
        "                G, pos, ax=ax,\n",
        "                with_labels=True,\n",
        "                node_size=node_sizes,\n",
        "                node_color=\"skyblue\",\n",
        "                font_size=10,\n",
        "                edge_color=\"gray\",\n",
        "                arrows=True,\n",
        "                arrowsize=15\n",
        "            )\n",
        "\n",
        "            ax.set_title(title)\n",
        "            ax.axis('off')\n",
        "\n",
        "            return fig\n",
        "        except:\n",
        "            # Fallback if network drawing fails\n",
        "            ax.text(0.5, 0.5, \"Could not generate network graph from available data\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "    def generate_scatter_plot(self, df: pd.DataFrame, x_col: str, y_col: str,\n",
        "                             color_col: str = None, title: str = \"Scatter Analysis\") -> plt.Figure:\n",
        "        \"\"\"Generate scatter plot to identify relationships and outliers\"\"\"\n",
        "        if x_col not in df.columns or y_col not in df.columns or df.empty:\n",
        "            # Create empty plot with message\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            ax.text(0.5, 0.5, \"Insufficient data for scatter plot\",\n",
        "                   ha='center', va='center', fontsize=14)\n",
        "            ax.set_title(title)\n",
        "            return fig\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        if color_col and color_col in df.columns:\n",
        "            scatter = ax.scatter(df[x_col], df[y_col], c=df[color_col], cmap='viridis',\n",
        "                               alpha=0.7, s=70, edgecolors='w')\n",
        "            plt.colorbar(scatter, ax=ax, label=color_col)\n",
        "        else:\n",
        "            ax.scatter(df[x_col], df[y_col], alpha=0.7, s=70, edgecolors='w')\n",
        "\n",
        "        # Add trendline\n",
        "        try:\n",
        "            z = np.polyfit(df[x_col], df[y_col], 1)\n",
        "            p = np.poly1d(z)\n",
        "            ax.plot(df[x_col], p(df[x_col]), \"r--\", alpha=0.8, label=f\"Trend: y={z[0]:.2f}x+{z[1]:.2f}\")\n",
        "            ax.legend()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel(x_col)\n",
        "        ax.set_ylabel(y_col)\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        fig.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def generate_visualizations(self, query_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Generate visualizations based on anomaly detection results\"\"\"\n",
        "        # Extract text chunks for data extraction\n",
        "        text_chunks = [result[\"text\"] for result in query_results[\"top_results\"]]\n",
        "\n",
        "        # Extract structured data from text\n",
        "        structured_data = self.extract_structured_data(text_chunks)\n",
        "\n",
        "        # Get visualization suggestions based on data and analysis\n",
        "        vis_suggestions = self.suggest_visualizations(\n",
        "            structured_data,\n",
        "            query_results[\"anomaly_analysis\"]\n",
        "        )\n",
        "\n",
        "        # Generate visualizations\n",
        "        visualizations = []\n",
        "\n",
        "        for suggestion in vis_suggestions:\n",
        "            vis_type = suggestion.get(\"type\", \"\").lower()\n",
        "            columns = suggestion.get(\"columns\", [])\n",
        "            purpose = suggestion.get(\"purpose\", \"Supply Chain Analysis\")\n",
        "            anomaly_threshold = suggestion.get(\"anomaly_threshold\")\n",
        "\n",
        "            if len(columns) < 2:\n",
        "                continue\n",
        "\n",
        "            if vis_type == \"time_series\":\n",
        "                fig = self.generate_time_series(\n",
        "                    structured_data,\n",
        "                    columns[0],\n",
        "                    columns[1],\n",
        "                    purpose,\n",
        "                    anomaly_threshold\n",
        "                )\n",
        "                visualizations.append({\"type\": vis_type, \"figure\": fig, \"purpose\": purpose})\n",
        "\n",
        "            elif vis_type == \"bar_chart\":\n",
        "                fig = self.generate_bar_chart(\n",
        "                    structured_data,\n",
        "                    columns[0],\n",
        "                    columns[1],\n",
        "                    purpose,\n",
        "                    anomaly_threshold\n",
        "                )\n",
        "                visualizations.append({\"type\": vis_type, \"figure\": fig, \"purpose\": purpose})\n",
        "\n",
        "            elif vis_type == \"heatmap\" and len(columns) >= 3:\n",
        "                fig = self.generate_heatmap(\n",
        "                    structured_data,\n",
        "                    columns[0],\n",
        "                    columns[1],\n",
        "                    columns[2],\n",
        "                    purpose\n",
        "                )\n",
        "                visualizations.append({\"type\": vis_type, \"figure\": fig, \"purpose\": purpose})\n",
        "\n",
        "            elif vis_type == \"network\" and len(columns) >= 2:\n",
        "                weight_col = columns[2] if len(columns) > 2 else None\n",
        "                fig = self.generate_network_graph(\n",
        "                    structured_data,\n",
        "                    columns[0],\n",
        "                    columns[1],\n",
        "                    weight_col,\n",
        "                    purpose\n",
        "                )\n",
        "                visualizations.append({\"type\": vis_type, \"figure\": fig, \"purpose\": purpose})\n",
        "\n",
        "            elif vis_type == \"scatter\":\n",
        "                color_col = columns[2] if len(columns) > 2 else None\n",
        "                fig = self.generate_scatter_plot(\n",
        "                    structured_data,\n",
        "                    columns[0],\n",
        "                    columns[1],\n",
        "                    color_col,\n",
        "                    purpose\n",
        "                )\n",
        "                visualizations.append({\"type\": vis_type, \"figure\": fig, \"purpose\": purpose})\n",
        "\n",
        "        # Add the structured data and visualizations to the results\n",
        "        query_results[\"structured_data\"] = structured_data.to_dict(orient='records')\n",
        "        query_results[\"visualizations\"] = visualizations\n",
        "\n",
        "        return query_results\n",
        "\n",
        "    def extract_all_data_as_json(self, full_text_per_page: List[str]) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Extracts structured data from all identified tables in the document text\n",
        "        and returns it as a dictionary of lists of dictionaries (JSON-ready).\n",
        "        (Placed inside the class)\n",
        "\n",
        "        Args:\n",
        "            full_text_per_page (List[str]): List containing the full text of each page.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, List[Dict]]: A dictionary where keys are table names\n",
        "                                   (e.g., 'deliveryPerformance') and values are lists\n",
        "                                   of dictionaries representing table rows.\n",
        "        \"\"\"\n",
        "        structured_data = {\n",
        "            \"deliveryPerformance\": [], \"supplierQuality\": [], \"carbonFootprint\": [],\n",
        "            \"networkConnectivity\": [], \"priceFluctuations\": [], \"inventoryLevels\": [],\n",
        "            \"anomalyNotes\": [], \"recommendations\": [],\n",
        "        }\n",
        "        full_doc_text = \"\\n\".join(full_text_per_page) # Combine page text\n",
        "        print(\"[JSON Extraction] Starting comprehensive JSON data extraction...\")\n",
        "\n",
        "        # --- Delivery Performance Extraction ---\n",
        "        print(\"\\n--- Debugging Delivery Performance ---\")\n",
        "        try:\n",
        "            delivery_match = re.search(\n",
        "                r\"DELIVERY PERFORMANCE\\s*\\n(.*?)\\n\\s*(?:SUPPLIER QUALITY ASSESSMENT|CARBON FOOTPRINT ANALYSIS|SUPPLY CHAIN NETWORK CONNECTIVITY|$)\",\n",
        "                full_doc_text, re.DOTALL | re.IGNORECASE\n",
        "            )\n",
        "            if delivery_match:\n",
        "                table_text = delivery_match.group(1).strip()\n",
        "                lines = [line.strip() for line in table_text.split('\\n') if line.strip()]\n",
        "\n",
        "                # --- Debugging: Print the identified table text ---\n",
        "                # print(f\"[DEBUG Delivery] Raw Table Text Block:\\n---\\n{table_text}\\n---\")\n",
        "                # print(f\"[DEBUG Delivery] Split into {len(lines)} lines.\")\n",
        "                # --- End Debugging ---\n",
        "\n",
        "                if len(lines) > 1:\n",
        "                    print(f\"[DEBUG Delivery] Potential Header Line(s)? : '{lines[0]}'\")\n",
        "                    print(f\"[DEBUG Delivery] Attempting to parse {len(lines)-1} potential data lines...\")\n",
        "                    line_num = 0 # Keep track of line number outside the inner try/except\n",
        "                    parse_errors_count = 0\n",
        "                    success_count = 0\n",
        "\n",
        "                    for line in lines[1:]: # Skip header line(s)\n",
        "                        line_num += 1\n",
        "                        print(f\"\\n[DEBUG Delivery] Processing Line {line_num}: '{line}'\") # <<< PRINT THE RAW LINE\n",
        "                        parts = line.split() # Basic split\n",
        "                        print(f\"[DEBUG Delivery] Parts ({len(parts)}): {parts}\") # <<< PRINT THE SPLIT PARTS\n",
        "\n",
        "                        if len(parts) >= 5: # Basic check for minimum expected columns\n",
        "                            try:\n",
        "                                # --- Attempt to parse ---\n",
        "                                date = parts[0]\n",
        "                                supplier = parts[1]\n",
        "                                # Find numeric parts from the end more reliably\n",
        "                                if not parts[-1].replace('+','').replace('-','').isdigit(): raise ValueError(\"Variance not numeric\")\n",
        "                                if not parts[-2].replace('O','0').isdigit(): raise ValueError(\"ExpectedTime not numeric\")\n",
        "                                if not parts[-3].replace('O','0').isdigit(): raise ValueError(\"DeliveryTime not numeric\")\n",
        "\n",
        "                                variance_str = parts[-1]\n",
        "                                expected_time = int(parts[-2].replace('O','0')) # Handle O/0\n",
        "                                delivery_time = int(parts[-3].replace('O','0')) # Handle O/0\n",
        "\n",
        "                                # Assume product is everything between supplier (index 1) and delivery_time (index -3)\n",
        "                                product = \" \".join(parts[2:-3]).strip()\n",
        "                                if not product: product = \"Unknown\" # Handle case where join results in empty\n",
        "\n",
        "                                variance = int(variance_str.replace('+', '')) if variance_str else 0\n",
        "\n",
        "                                row = {\"Date\": date, \"Supplier\": supplier, \"Product\": product, \"DeliveryTime\": delivery_time, \"ExpectedTime\": expected_time, \"Variance\": variance}\n",
        "                                print(f\"[DEBUG Delivery] Parsed Row: {row}\") # <<< PRINT SUCCESSFUL PARSE\n",
        "                                structured_data[\"deliveryPerformance\"].append(row)\n",
        "                                success_count += 1\n",
        "                            except (ValueError, IndexError) as parse_err:\n",
        "                                # <<< PRINT THE ERROR AND THE LINE THAT CAUSED IT >>>\n",
        "                                print(f\"[DEBUG Delivery] Skipping Delivery Perf line {line_num} due to parse error: {parse_err} --- Line was: '{line}'\")\n",
        "                                parse_errors_count += 1\n",
        "                        else:\n",
        "                            print(f\"[DEBUG Delivery] Skipping line {line_num}: Not enough parts (found {len(parts)}, expected >= 5).\")\n",
        "                            parse_errors_count += 1\n",
        "                    print(f\"[DEBUG Delivery] Finished processing lines. Success: {success_count}, Errors/Skipped: {parse_errors_count}\")\n",
        "\n",
        "                else: print(\"[JSON Extraction] Delivery Performance section found but no data lines after header.\")\n",
        "            else: print(\"[JSON Extraction] Warning: Delivery Performance regex pattern did not match.\")\n",
        "        except Exception as e:\n",
        "            # Use traceback for more detail if needed\n",
        "            # import traceback\n",
        "            # print(f\"[JSON Extraction] Error processing Delivery Performance section: {e}\\n{traceback.format_exc()}\")\n",
        "            print(f\"[JSON Extraction] UNEXPECTED Error processing Delivery Performance section: {e}\") # Avoid using loop variables here\n",
        "        print(\"--- End Debugging Delivery Performance ---\")\n",
        "\n",
        "        # --- Supplier Quality Assessment Extraction ---\n",
        "        try:\n",
        "            quality_match = re.search(\n",
        "                r\"SUPPLIER QUALITY ASSESSMENT\\s*\\n(.*?)\\n\\s*(?:CARBON FOOTPRINT ANALYSIS|SUPPLY CHAIN NETWORK CONNECTIVITY|$)\",\n",
        "                 full_doc_text, re.DOTALL | re.IGNORECASE\n",
        "            )\n",
        "            if quality_match:\n",
        "                table_text = quality_match.group(1).strip()\n",
        "                lines = [line.strip() for line in table_text.split('\\n') if line.strip()]\n",
        "                if len(lines) > 1:\n",
        "                    print(f\"[JSON Extraction] Found {len(lines)-1} lines in Supplier Quality section.\")\n",
        "                    for line in lines[1:]:\n",
        "                        parts = line.split()\n",
        "                        if len(parts) >= 5:\n",
        "                             try:\n",
        "                                rej_rate = float(parts[-1]); quantity = int(parts[-2].replace('O','0'))\n",
        "                                price = float(parts[-3].replace('O','0')); quality_score = int(parts[-4].replace('O','0'))\n",
        "                                product_start_index = 1 # Default\n",
        "                                potential_prod_starts = [\"Electronic\", \"Raw\", \"Component\"]\n",
        "                                for i, part in enumerate(parts[:-4]):\n",
        "                                    if any(part.startswith(p) for p in potential_prod_starts): product_start_index = i; break\n",
        "                                supplier = \" \".join(parts[:product_start_index]); product = \" \".join(parts[product_start_index:-4])\n",
        "                                row = {\"Supplier\": supplier, \"Product\": product, \"QualityScore\": quality_score, \"Price\": price, \"Quantity\": quantity, \"RejectionRate\": rej_rate}\n",
        "                                structured_data[\"supplierQuality\"].append(row)\n",
        "                             except (ValueError, IndexError) as parse_err: print(f\"[JSON Extraction] Skipping Supplier Quality line: '{line}' -> {parse_err}\")\n",
        "                else: print(\"[JSON Extraction] Supplier Quality section found but no data lines.\")\n",
        "            else: print(\"[JSON Extraction] Warning: Supplier Quality table not found.\")\n",
        "        except Exception as e: print(f\"[JSON Extraction] Error parsing Supplier Quality section: {e}\")\n",
        "\n",
        "        # --- Carbon Footprint Analysis Extraction ---\n",
        "        try:\n",
        "            carbon_match = re.search(\n",
        "                r\"CARBON FOOTPRINT ANALYSIS\\s*\\n(.*?)\\n\\s*(?:SUPPLY CHAIN NETWORK CONNECTIVITY|PRICE FLUCTUATIONS|$)\",\n",
        "                full_doc_text, re.DOTALL | re.IGNORECASE\n",
        "            )\n",
        "            if carbon_match:\n",
        "                table_text = carbon_match.group(1).strip()\n",
        "                lines = [line.strip() for line in table_text.split('\\n') if line.strip()]\n",
        "                if len(lines) > 1:\n",
        "                    print(f\"[JSON Extraction] Found {len(lines)-1} lines in Carbon Footprint section.\")\n",
        "                    # Skip header line (assuming it's the first)\n",
        "                    for line in lines[1:]:\n",
        "                        parts = line.split()\n",
        "                        if len(parts) >= 5:\n",
        "                            try:\n",
        "                                ship_weight = float(parts[-1].replace('O','0'))\n",
        "                                carbon_emissions = int(parts[-2].replace('O','0'))\n",
        "                                distance = int(parts[-3].replace('O','0'))\n",
        "                                mode = parts[1]; supplier = parts[0]\n",
        "                                row = {\"Supplier\": supplier, \"TransportMode\": mode, \"Distance\": distance, \"CarbonEmissions\": carbon_emissions, \"ShipmentWeight\": ship_weight}\n",
        "                                structured_data[\"carbonFootprint\"].append(row)\n",
        "                            except (ValueError, IndexError) as parse_err: print(f\"[JSON Extraction] Skipping Carbon Footprint line: '{line}' -> {parse_err}\")\n",
        "                else: print(\"[JSON Extraction] Carbon Footprint section found but no data lines.\")\n",
        "            else: print(\"[JSON Extraction] Warning: Carbon Footprint table not found.\")\n",
        "        except Exception as e: print(f\"[JSON Extraction] Error parsing Carbon Footprint section: {e}\")\n",
        "\n",
        "        # --- Supply Chain Network Connectivity Extraction ---\n",
        "        try:\n",
        "            conn_match = re.search(\n",
        "                r\"SUPPLY CHAIN NETWORK CONNECTIVITY\\s*\\n(.*?)\\n\\s*(?:PRICE FLUCTUATIONS|INVENTORY LEVELS|$)\",\n",
        "                full_doc_text, re.DOTALL | re.IGNORECASE\n",
        "            )\n",
        "            if conn_match:\n",
        "                table_text = conn_match.group(1).strip()\n",
        "                lines = [line.strip() for line in table_text.split('\\n') if line.strip()]\n",
        "                if len(lines) > 1:\n",
        "                    print(f\"[JSON Extraction] Found {len(lines)-1} lines in Network Connectivity section.\")\n",
        "                    for line in lines[1:]:\n",
        "                        parts = line.split()\n",
        "                        if len(parts) >= 5:\n",
        "                            try:\n",
        "                                transit_time = int(parts[-1]); reliability_score = int(parts[-2]); flow_volume = int(parts[-3])\n",
        "                                dest_start_index = len(parts) - 4 # Default: Assume Dest is one word\n",
        "                                potential_starts = [\"Distribution\", \"Warehouse\", \"Factory\"]\n",
        "                                for i in range(1, len(parts) - 3):\n",
        "                                    if any(parts[i].startswith(p) for p in potential_starts): dest_start_index = i; break\n",
        "                                source = \" \".join(parts[0:dest_start_index]); destination = \" \".join(parts[dest_start_index:-3])\n",
        "                                row = {\"Source\": source, \"Destination\": destination, \"FlowVolume\": flow_volume, \"ReliabilityScore\": reliability_score, \"TransitTime\": transit_time}\n",
        "                                structured_data[\"networkConnectivity\"].append(row)\n",
        "                            except (ValueError, IndexError) as parse_err: print(f\"[JSON Extraction] Skipping Network Connectivity line: '{line}' -> {parse_err}\")\n",
        "                else: print(\"[JSON Extraction] Network Connectivity section found but no data lines.\")\n",
        "            else: print(\"[JSON Extraction] Warning: Network Connectivity table not found.\")\n",
        "        except Exception as e: print(f\"[JSON Extraction] Error parsing Network Connectivity section: {e}\")\n",
        "\n",
        "        # --- Price Fluctuations Extraction ---\n",
        "        try:\n",
        "            price_match = re.search(\n",
        "                r\"PRICE FLUCTUATIONS\\s*\\n(.*?)\\n\\s*(?:INVENTORY LEVELS|ANOMALY NOTES|$)\",\n",
        "                full_doc_text, re.DOTALL | re.IGNORECASE\n",
        "            )\n",
        "            if price_match:\n",
        "                table_text = price_match.group(1).strip()\n",
        "                lines = [line.strip() for line in table_text.split('\\n') if line.strip()]\n",
        "                if len(lines) > 1:\n",
        "                     print(f\"[JSON Extraction] Found {len(lines)-1} lines in Price Fluctuations section.\")\n",
        "                     for line in lines[1:]:\n",
        "                        parts = line.split()\n",
        "                        if len(parts) >= 6:\n",
        "                            try:\n",
        "                                 currency = parts[-1]; deviation_str = parts[-2]\n",
        "                                 actual_price = float(parts[-3].replace('O','0')); base_price = float(parts[-4].replace('O','0'))\n",
        "                                 supplier = parts[-5]; product = \" \".join(parts[1:-5]); date = parts[0]\n",
        "                                 deviation = float(deviation_str.replace('%', '').replace('+', ''))\n",
        "                                 row = {\"Date\": date, \"Product\": product, \"Supplier\": supplier, \"BasePrice\": base_price, \"ActualPrice\": actual_price, \"Deviation\": deviation, \"Currency\": currency}\n",
        "                                 structured_data[\"priceFluctuations\"].append(row)\n",
        "                            except (ValueError, IndexError) as parse_err: print(f\"[JSON Extraction] Skipping Price Fluctuations line: '{line}' -> {parse_err}\")\n",
        "                else: print(\"[JSON Extraction] Price Fluctuations section found but no data lines.\")\n",
        "            else: print(\"[JSON Extraction] Warning: Price Fluctuations table not found.\")\n",
        "        except Exception as e: print(f\"[JSON Extraction] Error parsing Price Fluctuations section: {e}\")\n",
        "\n",
        "        # --- Inventory Levels Extraction ---\n",
        "        try:\n",
        "            inventory_match = re.search(\n",
        "                r\"INVENTORY LEVELS\\s*\\n(.*?)(?:\\n\\s*ANOMALY NOTES|\\Z)\",\n",
        "                full_doc_text, re.DOTALL | re.IGNORECASE\n",
        "            )\n",
        "            if inventory_match:\n",
        "                table_text = inventory_match.group(1).strip()\n",
        "                lines = [line.strip() for line in table_text.split('\\n') if line.strip()]\n",
        "                if len(lines) > 1:\n",
        "                    print(f\"[JSON Extraction] Found {len(lines)-1} lines in Inventory Levels section.\")\n",
        "                    for line in lines[1:]:\n",
        "                        parts = line.split()\n",
        "                        if len(parts) >= 7:\n",
        "                            try:\n",
        "                                reorder_point = int(parts[-1].replace('O','0')); days_supply = int(parts[-2].replace('O','0'))\n",
        "                                optimal_stock = int(parts[-3].replace('O','0')); current_stock = int(parts[-4].replace('O','0'))\n",
        "                                product = parts[-5]; warehouse = parts[-6]; date = parts[0]\n",
        "                                row = {\"Date\": date, \"Warehouse\": warehouse, \"Product\": product, \"CurrentStock\": current_stock, \"OptimalStock\": optimal_stock, \"DaysOfSupply\": days_supply, \"ReorderPoint\": reorder_point}\n",
        "                                structured_data[\"inventoryLevels\"].append(row)\n",
        "                            except (ValueError, IndexError) as parse_err: print(f\"[JSON Extraction] Skipping Inventory Levels line: '{line}' -> {parse_err}\")\n",
        "                else: print(\"[JSON Extraction] Inventory Levels section found but no data lines.\")\n",
        "            else: print(\"[JSON Extraction] Warning: Inventory Levels table not found.\")\n",
        "        except Exception as e: print(f\"[JSON Extraction] Error parsing Inventory Levels section: {e}\")\n",
        "\n",
        "        # --- Anomaly Notes / Recommendations Extraction (Text) ---\n",
        "        try:\n",
        "             anomaly_match = re.search(r\"ANOMALY NOTES\\s*\\n(.*?)\\n\\s*(?:RECOMMENDATIONS|\\Z)\", full_doc_text, re.DOTALL | re.IGNORECASE)\n",
        "             if anomaly_match:\n",
        "                 notes_text = anomaly_match.group(1).strip()\n",
        "                 notes_list = re.split(r'\\n\\s*\\d+\\.\\s*', notes_text)\n",
        "                 if notes_list and not notes_list[0]: notes_list.pop(0)\n",
        "                 structured_data[\"anomalyNotes\"] = [{\"id\": i+1, \"text\": note.strip()} for i, note in enumerate(notes_list) if note.strip()]\n",
        "                 print(f\"[JSON Extraction] Extracted {len(structured_data['anomalyNotes'])} anomaly notes.\")\n",
        "\n",
        "             recs_match = re.search(r\"RECOMMENDATIONS\\s*\\n(.*)\", full_doc_text, re.DOTALL | re.IGNORECASE)\n",
        "             if recs_match:\n",
        "                  recs_text = recs_match.group(1).strip()\n",
        "                  recs_list = re.split(r'\\n\\s*\\d+\\.\\s*', recs_text)\n",
        "                  if recs_list and not recs_list[0]: recs_list.pop(0)\n",
        "                  structured_data[\"recommendations\"] = [{\"id\": i+1, \"text\": rec.strip()} for i, rec in enumerate(recs_list) if rec.strip()]\n",
        "                  print(f\"[JSON Extraction] Extracted {len(structured_data['recommendations'])} recommendations.\")\n",
        "\n",
        "        except Exception as e: print(f\"[JSON Extraction] Error parsing Notes/Recommendations: {e}\")\n",
        "\n",
        "        print(\"[JSON Extraction] Finished comprehensive JSON data extraction.\")\n",
        "        return structured_data\n",
        "\n",
        "\n",
        "    # ======================== HELPER FUNCTIONS ========================\n",
        "def process_supply_chain_documents(folder_path: str) -> SupplyChainRAGSystem:\n",
        "    system = SupplyChainRAGSystem()\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            extracted_data = system.process_pdf(file_path)\n",
        "            chunks = system.chunk_document(extracted_data)\n",
        "            system.index_chunks(chunks)\n",
        "    return system\n",
        "\n",
        "def run_anomaly_detection(system: SupplyChainRAGSystem, query: str, generate_graphs: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run anomaly detection with optional graph generation (Original version)\n",
        "    \"\"\"\n",
        "    results = system.detect_anomalies(query)\n",
        "    if generate_graphs:\n",
        "        # --- REMOVE THE DUPLICATE/NESTED DEFINITIONS FROM HERE ---\n",
        "        # The definitions for display_visualizations and the second\n",
        "        # extract_all_data_as_json that were previously here are REMOVED.\n",
        "\n",
        "        # Call the method on the system instance passed in\n",
        "        try:\n",
        "             results = system.generate_visualizations(results) # Calls the plotting version\n",
        "\n",
        "             # Save visualizations to files if needed\n",
        "             if \"visualizations\" in results:\n",
        "                 os.makedirs(\"supply_chain_visualizations\", exist_ok=True)\n",
        "                 for i, vis in enumerate(results[\"visualizations\"]):\n",
        "                     if \"figure\" in vis and isinstance(vis[\"figure\"], plt.Figure): # Check if it's a figure\n",
        "                         filename = f\"supply_chain_visualizations/{vis['type']}_{i}.png\"\n",
        "                         try:\n",
        "                             vis[\"figure\"].savefig(filename)\n",
        "                             vis[\"image_path\"] = filename\n",
        "                             plt.close(vis[\"figure\"]) # Close figure after saving to free memory\n",
        "                         except Exception as save_err:\n",
        "                              print(f\"Error saving figure {filename}: {save_err}\")\n",
        "                     # Remove the figure object from results to make it JSON serializable if needed later\n",
        "                     if \"figure\" in vis:\n",
        "                          del vis[\"figure\"]\n",
        "\n",
        "        except Exception as viz_gen_err:\n",
        "            print(f\"Error calling generate_visualizations: {viz_gen_err}\")\n",
        "            if \"visualizations\" not in results: results[\"visualizations\"] = []\n",
        "            if \"structured_data\" not in results: results[\"structured_data\"] = []\n",
        "\n",
        "\n",
        "    # Print analysis only (keep this)\n",
        "    print(\"\\n--- Anomaly Analysis (from run_anomaly_detection) ---\")\n",
        "    print(results.get(\"anomaly_analysis\", \"Analysis not available.\"))\n",
        "    print(\"--- End Anomaly Analysis ---\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def display_visualizations(results: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Display all generated visualizations (Original function, placed correctly)\n",
        "    NOTE: This will likely NOT work correctly now because figure objects are removed\n",
        "          in run_anomaly_detection after saving. This is kept for structure but\n",
        "          displaying plots directly in a backend script running for an API isn't typical.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Attempting to Display Visualizations (Note: May not work if figures were closed) ---\")\n",
        "    if \"visualizations\" in results and results[\"visualizations\"]:\n",
        "        displayed_count = 0\n",
        "        for vis in results[\"visualizations\"]:\n",
        "            # Figure object might have been deleted after saving\n",
        "            if \"figure\" in vis and vis[\"figure\"] is not None:\n",
        "                 try:\n",
        "                    plt.figure(vis[\"figure\"].number) # This might error if closed\n",
        "                    plt.title(vis[\"purpose\"])\n",
        "                    plt.show() # plt.show() blocks execution in scripts, not ideal for backend\n",
        "                    displayed_count += 1\n",
        "                 except Exception as display_err:\n",
        "                     # print(f\"Could not display figure: {display_err}\") # Error expected if closed\n",
        "                     pass # Silently ignore if figure was closed\n",
        "            # If figure object is gone, maybe print the path where it was saved\n",
        "            elif \"image_path\" in vis:\n",
        "                 print(f\"Visualization for '{vis.get('purpose', 'N/A')}' saved to: {vis['image_path']}\")\n",
        "\n",
        "        if displayed_count == 0 and any(\"image_path\" in v for v in results[\"visualizations\"]):\n",
        "             print(\"Figures were saved to files (see paths above), not displayed interactively.\")\n",
        "        elif displayed_count == 0:\n",
        "             print(\"No valid figure objects found to display.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No visualizations key found in results.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ======================== USAGE EXAMPLE ========================\n",
        "# Example usage:\n",
        "# 1. Process documents\n",
        "# system = process_supply_chain_documents(\"supply_chain_docs\")\n",
        "#\n",
        "# 2. Run analysis with visualization\n",
        "# results = run_anomaly_detection(system, \"Are there any delivery delays with supplier XYZ?\")\n",
        "#\n",
        "# 3. Display visualizations\n",
        "# display_visualizations(results)\n",
        "\n",
        "\n",
        "def run_full_analysis_pipeline(system: SupplyChainRAGSystem, query: str, pdf_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full analysis pipeline:\n",
        "    1. Runs original RAG/Analysis/Graph generation (`run_anomaly_detection`).\n",
        "    2. Extracts comprehensive structured JSON data (`extract_all_data_as_json`).\n",
        "    3. Combines results for the frontend.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Full Analysis Pipeline ---\")\n",
        "    print(f\"\\nQuery: '{query}'\\nPDF Path: '{pdf_path}'\")\n",
        "\n",
        "    # == Step 1: Run Original Analysis & Visualization ==\n",
        "    print(\"\\nStep 1: Running original analysis and graph generation...\")\n",
        "    try:\n",
        "        # Run the original function which includes plotting\n",
        "        original_results = run_anomaly_detection(system, query, generate_graphs=True)\n",
        "        print(\"Step 1: Original analysis and graph generation complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Step 1 (run_anomaly_detection): {e}\")\n",
        "        # Create a fallback structure\n",
        "        original_results = {\n",
        "            \"query\": query, \"anomaly_analysis\": f\"Error during analysis: {e}\",\n",
        "            \"visualizations\": [], \"structured_data\": [], \"hyde_document\": \"N/A\", \"top_results\": [] }\n",
        "\n",
        "    # == Step 2: Extract Comprehensive Structured JSON Data ==\n",
        "    print(\"\\nStep 2: Extracting comprehensive structured data as JSON...\")\n",
        "    structured_json_data = {\"error\": \"Extraction not run or failed\"} # Default\n",
        "    try:\n",
        "        print(f\"Processing PDF for JSON extraction: {pdf_path}...\")\n",
        "        extracted_pages_for_json = system.process_pdf(pdf_path) # Gets original_text\n",
        "        full_text_per_page = [page.get('original_text', '') for page in extracted_pages_for_json]\n",
        "\n",
        "        if not any(p.strip() for p in full_text_per_page):\n",
        "             print(\"ERROR: No original text found after processing PDF for JSON extraction.\")\n",
        "             raise ValueError(\"PDF processing yielded no text for JSON extraction.\")\n",
        "\n",
        "        # Call the comprehensive extraction METHOD on the system INSTANCE\n",
        "        structured_json_data = system.extract_all_data_as_json(full_text_per_page)\n",
        "        print(\"Step 2: Comprehensive JSON data extraction complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during comprehensive JSON extraction (Step 2): {e}\")\n",
        "        # Ensure structured_json_data retains the error message\n",
        "        if isinstance(structured_json_data, dict): # Avoid overwriting existing error\n",
        "             structured_json_data[\"error_step2\"] = f\"Failed to extract JSON data: {e}\"\n",
        "        else:\n",
        "             structured_json_data = {\"error\": f\"Failed to extract JSON data: {e}\"}\n",
        "\n",
        "\n",
        "    # == Step 3: Combine Results ==\n",
        "    print(\"\\nStep 3: Combining results...\")\n",
        "    # Ensure keys exist even if steps failed\n",
        "    final_analysis_text = original_results.get(\"anomaly_analysis\", \"Analysis missing or failed.\")\n",
        "    final_visualizations = original_results.get(\"visualizations\", [])\n",
        "\n",
        "    # Get notes/recs from the new JSON data if extraction was successful\n",
        "    final_anomaly_notes = []\n",
        "    final_recommendations = []\n",
        "    if isinstance(structured_json_data, dict) and \"error\" not in structured_json_data:\n",
        "         final_anomaly_notes = structured_json_data.get(\"anomalyNotes\", [])\n",
        "         final_recommendations = structured_json_data.get(\"recommendations\", [])\n",
        "         # Optionally remove them from the main JSON block if added separately\n",
        "         # if \"anomalyNotes\" in structured_json_data: del structured_json_data[\"anomalyNotes\"]\n",
        "         # if \"recommendations\" in structured_json_data: del structured_json_data[\"recommendations\"]\n",
        "\n",
        "\n",
        "    final_output = {\n",
        "        \"analysisText\": final_analysis_text,\n",
        "        \"visualizations\": final_visualizations, # From original run\n",
        "        \"structuredDataJSON\": structured_json_data, # New comprehensive data (or error dict)\n",
        "        \"query\": original_results.get(\"query\", query),\n",
        "        \"anomalyNotes\": final_anomaly_notes, # Extracted separately now\n",
        "        \"recommendations\": final_recommendations # Extracted separately now\n",
        "    }\n",
        "    print(\"--- Full Analysis Pipeline Finished ---\")\n",
        "    return final_output\n",
        "\n",
        "\n",
        "# --- How you would use it (e.g., in your API endpoint handler) ---\n",
        "# pdf_file = \"path/to/uploaded/file.pdf\"\n",
        "# user_query = \"Analyze Q1 performance.\"\n",
        "\n",
        "# # Initialize the system (maybe once when the app starts or per request)\n",
        "# rag_system_instance = SupplyChainRAGSystem()\n",
        "# # ... index documents if needed ...\n",
        "\n",
        "# # Call the standalone function\n",
        "# api_response_data = run_analysis_and_extract_json(rag_system_instance, user_query, pdf_file)\n",
        "\n",
        "# # Return api_response_data from your Flask/FastAPI/Django endpoint\n",
        "# # return jsonify(api_response_data) # Example for Flask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HygcEzSv59BH",
        "outputId": "0e4cffb2-194d-438b-f318-2dcfc38065e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAWe5EmcArFK21WosWTnRGef3hPsh8RbkU\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "pvEGsTT2vbVY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Sample Supply Chain Document for RAG and Visualization.pdf\"\n",
        "query = \"Provide a full analysis and data overview for Q1 2025.\" # Changed query slightly\n",
        "\n",
        "# Step 1: Initialize the system\n",
        "print(\"Initializing system...\")\n",
        "system = SupplyChainRAGSystem()\n",
        "print(\"System initialized.\")\n",
        "\n",
        "# Step 2: Process & Index (using the corrected functions)\n",
        "print(f\"\\nProcessing PDF for indexing: {pdf_path}...\")\n",
        "extracted_pages_for_indexing = system.process_pdf(pdf_path)\n",
        "print(\"PDF processed for indexing.\")\n",
        "\n",
        "if not extracted_pages_for_indexing:\n",
        "    print(\"FATAL ERROR: No text could be extracted from the PDF for indexing. Stopping.\")\n",
        "else:\n",
        "    print(\"\\nChunking document...\")\n",
        "    chunks = system.chunk_document(extracted_pages_for_indexing)\n",
        "    print(f\"Document chunked into {len(chunks)} chunks.\")\n",
        "\n",
        "    if not chunks:\n",
        "        print(\"Warning: No chunks were generated after cleaning. RAG may not work well.\")\n",
        "\n",
        "    print(\"\\nIndexing chunks...\")\n",
        "    try:\n",
        "        system.index_chunks(chunks)\n",
        "        print(\"Chunks indexed successfully.\")\n",
        "\n",
        "        # Step 3: Run the NEW combined pipeline function\n",
        "        print(\"\\nRunning the full analysis pipeline...\")\n",
        "        api_response_data = run_full_analysis_pipeline(system, query, pdf_path)\n",
        "\n",
        "        # Step 4: Verification\n",
        "        print(\"\\n\\n--- Verifying structuredDataJSON Content ---\")\n",
        "        if \"structuredDataJSON\" not in api_response_data:\n",
        "            print(\"ERROR: 'structuredDataJSON' key is missing from the results.\")\n",
        "        else:\n",
        "            json_data = api_response_data[\"structuredDataJSON\"]\n",
        "            if not isinstance(json_data, dict): print(f\"ERROR: 'structuredDataJSON' is not a dictionary, type: {type(json_data)}\")\n",
        "            elif \"error\" in json_data: print(f\"ERROR: JSON Extraction failed: {json_data.get('error_step2') or json_data.get('error')}\")\n",
        "            else:\n",
        "                print(\"Found 'structuredDataJSON' dictionary. Checking contents...\")\n",
        "                expected_keys = [\"deliveryPerformance\", \"supplierQuality\", \"carbonFootprint\", \"networkConnectivity\", \"priceFluctuations\", \"inventoryLevels\", \"anomalyNotes\", \"recommendations\"]\n",
        "                all_good = True\n",
        "                for key in expected_keys:\n",
        "                    print(f\"\\nChecking '{key}':\")\n",
        "                    if key not in json_data: print(f\"  WARNING: Key '{key}' missing.\"); all_good = False\n",
        "                    else:\n",
        "                        data_list = json_data[key]\n",
        "                        if not isinstance(data_list, list): print(f\"  ERROR: Value for '{key}' not a list.\"); all_good = False\n",
        "                        elif not data_list: print(f\"  INFO: List for '{key}' is empty.\")\n",
        "                        else:\n",
        "                            print(f\"  SUCCESS: Found {len(data_list)} records.\")\n",
        "                            print(f\"  Sample record(s):\")\n",
        "                            try: print(json.dumps(data_list[:2], indent=2))\n",
        "                            except Exception as print_err: print(f\"  Error printing sample: {print_err}\"); all_good = False\n",
        "                print(\"\\n--- Verification Summary ---\")\n",
        "                if all_good: print(\"Basic verification PASSED. Visually inspect samples.\")\n",
        "                else: print(\"Verification encountered WARNINGS or ERRORS.\")\n",
        "\n",
        "        # --- Keep Graphviz example if desired ---\n",
        "        print(\"\\n--- Running Graphviz Example ---\")\n",
        "        def create_recommendation_map(root_issue, recommendations):\n",
        "            dot = Digraph(comment='Recommendation Map')\n",
        "            dot.attr(rankdir='LR'); dot.attr('node', shape='box', style='rounded,filled', color='lightblue2')\n",
        "            dot.node('root', root_issue, shape='ellipse', color='lightcoral')\n",
        "            for i, rec in enumerate(recommendations): dot.node(f'rec_{i}', rec); dot.edge('root', f'rec_{i}')\n",
        "            filename = dot.render('recommendation_map', format='png', cleanup=True)\n",
        "            print(f\"Recommendation map saved to {filename}\")\n",
        "\n",
        "        root_issue = \"Supply Chain Inefficiencies Q1 2025\"\n",
        "        recommendations_list = [item['text'] for item in api_response_data.get('recommendations', []) if 'text' in item]\n",
        "        if not recommendations_list: recommendations_list = [\"Review ExpressShip\", \"Improve Quality\", \"Assess Air Freight\", \"Enhance Inventory\", \"Investigate Pricing\"]\n",
        "        create_recommendation_map(root_issue, recommendations_list)\n",
        "\n",
        "    except Exception as index_err:\n",
        "        print(f\"ERROR during chunk indexing: {index_err}\")"
      ],
      "metadata": {
        "id": "kzkqwFKrsKbI",
        "outputId": "b9e1e7dd-f825-4c9c-a677-e7f92299bcab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing system...\n",
            "System initialized.\n",
            "\n",
            "Processing PDF for indexing: /content/Sample Supply Chain Document for RAG and Visualization.pdf...\n",
            "process_pdf extracted text from 3 pages using 'blocks' mode.\n",
            "PDF processed for indexing.\n",
            "\n",
            "Chunking document...\n",
            "Document chunked into 6 chunks.\n",
            "\n",
            "Indexing chunks...\n",
            "Chunks indexed successfully.\n",
            "\n",
            "Running the full analysis pipeline...\n",
            "--- Starting Full Analysis Pipeline ---\n",
            "\n",
            "Query: 'Provide a full analysis and data overview for Q1 2025.'\n",
            "PDF Path: '/content/Sample Supply Chain Document for RAG and Visualization.pdf'\n",
            "\n",
            "Step 1: Running original analysis and graph generation...\n",
            "Error in LLM JSON parsing: Expecting value: line 1 column 1 (char 0)\n",
            "Using hardcoded supplier quality data as fallback\n",
            "Generating bar chart for supplier vs quality_score\n",
            "DataFrame columns: ['supplier', 'product', 'quality_score']\n",
            "DataFrame shape: (8, 3)\n",
            "First few rows:\n",
            "      supplier         product  quality_score\n",
            "0   TechSupply    Electronic A             92\n",
            "1   PrimeParts    Electronic B             88\n",
            "2   GlobalComp    Electronic C             79\n",
            "3  IndustrialX  Raw Material D             90\n",
            "4   MetalWorks  Raw Material E             82\n",
            "Creating plot with 8 rows of data\n",
            "\n",
            "--- Anomaly Analysis (from run_anomaly_detection) ---\n",
            "## Supply Chain Analysis: Q1 2025\n",
            "\n",
            "**1. Delivery Delays or Timeline Inconsistencies:**\n",
            "\n",
            "* **ExpressShip:** Consistently exceeds expected delivery times by 7-10 days, particularly for Raw Material C. This suggests a potential systemic issue with ExpressShip's operations or a specific problem with their handling of Raw Material C.\n",
            "* **FastFreight:** Shows varied delivery times, with some deliveries exceeding expected times by 4-9 days. This requires further investigation to determine if these are isolated events or a pattern.\n",
            "* **GlobalTrans:** While some deliveries are on time, others show delays, with a maximum delay of +2 days.  Further analysis is needed to understand the causes of these delays.\n",
            "\n",
            "**2. Price Fluctuations Outside Normal Ranges:**\n",
            "\n",
            "* **Material Z:** Shows the highest price increase (10.9%) in February from MetalWorks. This significant deviation requires investigation into the root cause, including potential market factors, supply disruptions, or supplier negotiation tactics.\n",
            "\n",
            "**3. Quantity Discrepancies:** \n",
            "\n",
            "* No clear quantity discrepancies are explicitly mentioned in the provided information.\n",
            "\n",
            "**4. Quality Issues:**\n",
            "\n",
            "* **GlobalComp:**  Has a quality score below the acceptable threshold of 85% (79), indicating a potential problem with their products. \n",
            "* **FastSupply:** Also falls below the acceptable quality threshold (76). This suggests quality issues with their products and requires immediate attention.\n",
            "\n",
            "**5. Supplier Performance Problems:**\n",
            "\n",
            "* **ExpressShip:**  Poor delivery performance, particularly with Raw Material C. This suggests a need for a performance review and potentially exploring alternative suppliers for Raw Material C.\n",
            "* **GlobalComp & FastSupply:**  Subpar quality scores raise concerns about their ability to meet quality standards. Implementing quality improvement programs with these suppliers is recommended.\n",
            "* **MetalWorks:** Significant price increase for Material Z requires investigation and potentially negotiating long-term pricing agreements for better price stability.\n",
            "\n",
            "**6. Logistical Bottlenecks:**\n",
            "\n",
            "* **Warehouse A - Product 2 & Warehouse B - Product 1:** Inventory levels dropped below reorder points in March. This suggests inadequate inventory management and potential supply disruptions. Improving inventory management systems is crucial to prevent future stockouts.\n",
            "\n",
            "**7. Carbon Footprint:**\n",
            "\n",
            "* **Air Freight:**  Carbon emissions from air freight are significantly higher than other transport modes without corresponding delivery time benefits. This raises concerns about environmental impact. Evaluating alternative transportation options for non-urgent shipments is recommended.\n",
            "\n",
            "**Overall Analysis:**\n",
            "\n",
            "Q1 2025 presents several challenges for the supply chain.  Delivery delays, quality issues, supplier performance problems, and inventory management weaknesses are all areas requiring attention. The high carbon footprint from air freight is also a concern.  Addressing these issues proactively will be critical to optimizing the supply chain's efficiency, sustainability, and overall performance.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "* Conduct performance reviews with problematic suppliers, such as ExpressShip, GlobalComp, and FastSupply, and consider alternative suppliers for specific products.\n",
            "* Implement quality improvement programs with GlobalComp and FastSupply to address their below-standard quality scores.\n",
            "* Review air freight usage and assess alternative transportation options for non-urgent shipments to reduce carbon footprint.\n",
            "* Improve inventory management systems to prevent stock levels from dropping below reorder points.\n",
            "* Investigate the price increase cause for Material Z and negotiate long-term pricing agreements with MetalWorks.\n",
            "\n",
            "By addressing these issues, the supply chain can improve its performance, mitigate risks, and ensure the efficient delivery of products to meet customer demands. \n",
            "\n",
            "--- End Anomaly Analysis ---\n",
            "Step 1: Original analysis and graph generation complete.\n",
            "\n",
            "Step 2: Extracting comprehensive structured data as JSON...\n",
            "Processing PDF for JSON extraction: /content/Sample Supply Chain Document for RAG and Visualization.pdf...\n",
            "process_pdf extracted text from 3 pages using 'blocks' mode.\n",
            "[JSON Extraction] Starting comprehensive JSON data extraction...\n",
            "\n",
            "--- Debugging Delivery Performance ---\n",
            "[DEBUG Delivery] Potential Header Line(s)? : 'Date'\n",
            "[DEBUG Delivery] Attempting to parse 77 potential data lines...\n",
            "\n",
            "[DEBUG Delivery] Processing Line 1: 'Supplier'\n",
            "[DEBUG Delivery] Parts (1): ['Supplier']\n",
            "[DEBUG Delivery] Skipping line 1: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 2: 'Product'\n",
            "[DEBUG Delivery] Parts (1): ['Product']\n",
            "[DEBUG Delivery] Skipping line 2: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 3: 'Delivery Time (days)'\n",
            "[DEBUG Delivery] Parts (3): ['Delivery', 'Time', '(days)']\n",
            "[DEBUG Delivery] Skipping line 3: Not enough parts (found 3, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 4: 'Expected Time (days)'\n",
            "[DEBUG Delivery] Parts (3): ['Expected', 'Time', '(days)']\n",
            "[DEBUG Delivery] Skipping line 4: Not enough parts (found 3, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 5: 'Variance'\n",
            "[DEBUG Delivery] Parts (1): ['Variance']\n",
            "[DEBUG Delivery] Skipping line 5: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 6: '2025-01-05'\n",
            "[DEBUG Delivery] Parts (1): ['2025-01-05']\n",
            "[DEBUG Delivery] Skipping line 6: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 7: 'FastFreight'\n",
            "[DEBUG Delivery] Parts (1): ['FastFreight']\n",
            "[DEBUG Delivery] Skipping line 7: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 8: 'Raw Material A'\n",
            "[DEBUG Delivery] Parts (3): ['Raw', 'Material', 'A']\n",
            "[DEBUG Delivery] Skipping line 8: Not enough parts (found 3, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 9: '12'\n",
            "[DEBUG Delivery] Parts (1): ['12']\n",
            "[DEBUG Delivery] Skipping line 9: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 10: '8'\n",
            "[DEBUG Delivery] Parts (1): ['8']\n",
            "[DEBUG Delivery] Skipping line 10: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 11: '+4'\n",
            "[DEBUG Delivery] Parts (1): ['+4']\n",
            "[DEBUG Delivery] Skipping line 11: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 12: '2025-01-12'\n",
            "[DEBUG Delivery] Parts (1): ['2025-01-12']\n",
            "[DEBUG Delivery] Skipping line 12: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 13: 'GlobalTrans'\n",
            "[DEBUG Delivery] Parts (1): ['GlobalTrans']\n",
            "[DEBUG Delivery] Skipping line 13: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 14: 'Component B'\n",
            "[DEBUG Delivery] Parts (2): ['Component', 'B']\n",
            "[DEBUG Delivery] Skipping line 14: Not enough parts (found 2, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 15: '10'\n",
            "[DEBUG Delivery] Parts (1): ['10']\n",
            "[DEBUG Delivery] Skipping line 15: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 16: '10'\n",
            "[DEBUG Delivery] Parts (1): ['10']\n",
            "[DEBUG Delivery] Skipping line 16: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 17: '0'\n",
            "[DEBUG Delivery] Parts (1): ['0']\n",
            "[DEBUG Delivery] Skipping line 17: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 18: '2025-01-18'\n",
            "[DEBUG Delivery] Parts (1): ['2025-01-18']\n",
            "[DEBUG Delivery] Skipping line 18: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 19: 'ExpressShip'\n",
            "[DEBUG Delivery] Parts (1): ['ExpressShip']\n",
            "[DEBUG Delivery] Skipping line 19: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 20: 'Raw Material C'\n",
            "[DEBUG Delivery] Parts (3): ['Raw', 'Material', 'C']\n",
            "[DEBUG Delivery] Skipping line 20: Not enough parts (found 3, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 21: '18'\n",
            "[DEBUG Delivery] Parts (1): ['18']\n",
            "[DEBUG Delivery] Skipping line 21: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 22: '9'\n",
            "[DEBUG Delivery] Parts (1): ['9']\n",
            "[DEBUG Delivery] Skipping line 22: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 23: '+9'\n",
            "[DEBUG Delivery] Parts (1): ['+9']\n",
            "[DEBUG Delivery] Skipping line 23: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 24: '2025-01-25'\n",
            "[DEBUG Delivery] Parts (1): ['2025-01-25']\n",
            "[DEBUG Delivery] Skipping line 24: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 25: 'FastFreight'\n",
            "[DEBUG Delivery] Parts (1): ['FastFreight']\n",
            "[DEBUG Delivery] Skipping line 25: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 26: 'Component D'\n",
            "[DEBUG Delivery] Parts (2): ['Component', 'D']\n",
            "[DEBUG Delivery] Skipping line 26: Not enough parts (found 2, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 27: '9'\n",
            "[DEBUG Delivery] Parts (1): ['9']\n",
            "[DEBUG Delivery] Skipping line 27: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 28: '8'\n",
            "[DEBUG Delivery] Parts (1): ['8']\n",
            "[DEBUG Delivery] Skipping line 28: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 29: '+1'\n",
            "[DEBUG Delivery] Parts (1): ['+1']\n",
            "[DEBUG Delivery] Skipping line 29: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 30: '2025-02-03'\n",
            "[DEBUG Delivery] Parts (1): ['2025-02-03']\n",
            "[DEBUG Delivery] Skipping line 30: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 31: 'LocalLogist'\n",
            "[DEBUG Delivery] Parts (1): ['LocalLogist']\n",
            "[DEBUG Delivery] Skipping line 31: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 32: 'Raw Material A'\n",
            "[DEBUG Delivery] Parts (3): ['Raw', 'Material', 'A']\n",
            "[DEBUG Delivery] Skipping line 32: Not enough parts (found 3, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 33: '7'\n",
            "[DEBUG Delivery] Parts (1): ['7']\n",
            "[DEBUG Delivery] Skipping line 33: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 34: '8'\n",
            "[DEBUG Delivery] Parts (1): ['8']\n",
            "[DEBUG Delivery] Skipping line 34: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 35: '-1'\n",
            "[DEBUG Delivery] Parts (1): ['-1']\n",
            "[DEBUG Delivery] Skipping line 35: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 36: '2025-02-10'\n",
            "[DEBUG Delivery] Parts (1): ['2025-02-10']\n",
            "[DEBUG Delivery] Skipping line 36: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 37: 'ExpressShip'\n",
            "[DEBUG Delivery] Parts (1): ['ExpressShip']\n",
            "[DEBUG Delivery] Skipping line 37: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 38: 'Component B'\n",
            "[DEBUG Delivery] Parts (2): ['Component', 'B']\n",
            "[DEBUG Delivery] Skipping line 38: Not enough parts (found 2, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 39: '19'\n",
            "[DEBUG Delivery] Parts (1): ['19']\n",
            "[DEBUG Delivery] Skipping line 39: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 40: '9'\n",
            "[DEBUG Delivery] Parts (1): ['9']\n",
            "[DEBUG Delivery] Skipping line 40: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 41: '+10'\n",
            "[DEBUG Delivery] Parts (1): ['+10']\n",
            "[DEBUG Delivery] Skipping line 41: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 42: '2025-02-17'\n",
            "[DEBUG Delivery] Parts (1): ['2025-02-17']\n",
            "[DEBUG Delivery] Skipping line 42: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 43: 'GlobalTrans'\n",
            "[DEBUG Delivery] Parts (1): ['GlobalTrans']\n",
            "[DEBUG Delivery] Skipping line 43: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 44: 'Raw Material C'\n",
            "[DEBUG Delivery] Parts (3): ['Raw', 'Material', 'C']\n",
            "[DEBUG Delivery] Skipping line 44: Not enough parts (found 3, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 45: '12'\n",
            "[DEBUG Delivery] Parts (1): ['12']\n",
            "[DEBUG Delivery] Skipping line 45: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 46: '10'\n",
            "[DEBUG Delivery] Parts (1): ['10']\n",
            "[DEBUG Delivery] Skipping line 46: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 47: '+2'\n",
            "[DEBUG Delivery] Parts (1): ['+2']\n",
            "[DEBUG Delivery] Skipping line 47: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 48: '2025-02-24'\n",
            "[DEBUG Delivery] Parts (1): ['2025-02-24']\n",
            "[DEBUG Delivery] Skipping line 48: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 49: 'FastFreight'\n",
            "[DEBUG Delivery] Parts (1): ['FastFreight']\n",
            "[DEBUG Delivery] Skipping line 49: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 50: 'Component D'\n",
            "[DEBUG Delivery] Parts (2): ['Component', 'D']\n",
            "[DEBUG Delivery] Skipping line 50: Not enough parts (found 2, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 51: '13'\n",
            "[DEBUG Delivery] Parts (1): ['13']\n",
            "[DEBUG Delivery] Skipping line 51: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 52: '8'\n",
            "[DEBUG Delivery] Parts (1): ['8']\n",
            "[DEBUG Delivery] Skipping line 52: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 53: '+5'\n",
            "[DEBUG Delivery] Parts (1): ['+5']\n",
            "[DEBUG Delivery] Skipping line 53: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 54: '2025-03-05'\n",
            "[DEBUG Delivery] Parts (1): ['2025-03-05']\n",
            "[DEBUG Delivery] Skipping line 54: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 55: 'ExpressShip'\n",
            "[DEBUG Delivery] Parts (1): ['ExpressShip']\n",
            "[DEBUG Delivery] Skipping line 55: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 56: 'Raw Material A'\n",
            "[DEBUG Delivery] Parts (3): ['Raw', 'Material', 'A']\n",
            "[DEBUG Delivery] Skipping line 56: Not enough parts (found 3, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 57: '16'\n",
            "[DEBUG Delivery] Parts (1): ['16']\n",
            "[DEBUG Delivery] Skipping line 57: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 58: '9'\n",
            "[DEBUG Delivery] Parts (1): ['9']\n",
            "[DEBUG Delivery] Skipping line 58: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 59: '+7'\n",
            "[DEBUG Delivery] Parts (1): ['+7']\n",
            "[DEBUG Delivery] Skipping line 59: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 60: '2025-03-12'\n",
            "[DEBUG Delivery] Parts (1): ['2025-03-12']\n",
            "[DEBUG Delivery] Skipping line 60: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 61: 'LocalLogist'\n",
            "[DEBUG Delivery] Parts (1): ['LocalLogist']\n",
            "[DEBUG Delivery] Skipping line 61: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 62: 'Component B'\n",
            "[DEBUG Delivery] Parts (2): ['Component', 'B']\n",
            "[DEBUG Delivery] Skipping line 62: Not enough parts (found 2, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 63: '10'\n",
            "[DEBUG Delivery] Parts (1): ['10']\n",
            "[DEBUG Delivery] Skipping line 63: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 64: '8'\n",
            "[DEBUG Delivery] Parts (1): ['8']\n",
            "[DEBUG Delivery] Skipping line 64: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 65: '+2'\n",
            "[DEBUG Delivery] Parts (1): ['+2']\n",
            "[DEBUG Delivery] Skipping line 65: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 66: '2025-03-19'\n",
            "[DEBUG Delivery] Parts (1): ['2025-03-19']\n",
            "[DEBUG Delivery] Skipping line 66: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 67: 'GlobalTrans'\n",
            "[DEBUG Delivery] Parts (1): ['GlobalTrans']\n",
            "[DEBUG Delivery] Skipping line 67: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 68: 'Raw Material C'\n",
            "[DEBUG Delivery] Parts (3): ['Raw', 'Material', 'C']\n",
            "[DEBUG Delivery] Skipping line 68: Not enough parts (found 3, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 69: '11'\n",
            "[DEBUG Delivery] Parts (1): ['11']\n",
            "[DEBUG Delivery] Skipping line 69: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 70: '10'\n",
            "[DEBUG Delivery] Parts (1): ['10']\n",
            "[DEBUG Delivery] Skipping line 70: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 71: '+1'\n",
            "[DEBUG Delivery] Parts (1): ['+1']\n",
            "[DEBUG Delivery] Skipping line 71: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 72: '2025-03-26'\n",
            "[DEBUG Delivery] Parts (1): ['2025-03-26']\n",
            "[DEBUG Delivery] Skipping line 72: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 73: 'FastFreight'\n",
            "[DEBUG Delivery] Parts (1): ['FastFreight']\n",
            "[DEBUG Delivery] Skipping line 73: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 74: 'Component D'\n",
            "[DEBUG Delivery] Parts (2): ['Component', 'D']\n",
            "[DEBUG Delivery] Skipping line 74: Not enough parts (found 2, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 75: '14'\n",
            "[DEBUG Delivery] Parts (1): ['14']\n",
            "[DEBUG Delivery] Skipping line 75: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 76: '8'\n",
            "[DEBUG Delivery] Parts (1): ['8']\n",
            "[DEBUG Delivery] Skipping line 76: Not enough parts (found 1, expected >= 5).\n",
            "\n",
            "[DEBUG Delivery] Processing Line 77: '+6'\n",
            "[DEBUG Delivery] Parts (1): ['+6']\n",
            "[DEBUG Delivery] Skipping line 77: Not enough parts (found 1, expected >= 5).\n",
            "[DEBUG Delivery] Finished processing lines. Success: 0, Errors/Skipped: 77\n",
            "--- End Debugging Delivery Performance ---\n",
            "[JSON Extraction] Found 53 lines in Supplier Quality section.\n",
            "[JSON Extraction] Found 44 lines in Carbon Footprint section.\n",
            "[JSON Extraction] Found 49 lines in Network Connectivity section.\n",
            "[JSON Extraction] Found 48 lines in Price Fluctuations section.\n",
            "[JSON Extraction] Found 90 lines in Inventory Levels section.\n",
            "[JSON Extraction] Extracted 5 anomaly notes.\n",
            "[JSON Extraction] Extracted 5 recommendations.\n",
            "[JSON Extraction] Finished comprehensive JSON data extraction.\n",
            "Step 2: Comprehensive JSON data extraction complete.\n",
            "\n",
            "Step 3: Combining results...\n",
            "--- Full Analysis Pipeline Finished ---\n",
            "\n",
            "\n",
            "--- Verifying structuredDataJSON Content ---\n",
            "Found 'structuredDataJSON' dictionary. Checking contents...\n",
            "\n",
            "Checking 'deliveryPerformance':\n",
            "  INFO: List for 'deliveryPerformance' is empty.\n",
            "\n",
            "Checking 'supplierQuality':\n",
            "  INFO: List for 'supplierQuality' is empty.\n",
            "\n",
            "Checking 'carbonFootprint':\n",
            "  INFO: List for 'carbonFootprint' is empty.\n",
            "\n",
            "Checking 'networkConnectivity':\n",
            "  INFO: List for 'networkConnectivity' is empty.\n",
            "\n",
            "Checking 'priceFluctuations':\n",
            "  INFO: List for 'priceFluctuations' is empty.\n",
            "\n",
            "Checking 'inventoryLevels':\n",
            "  INFO: List for 'inventoryLevels' is empty.\n",
            "\n",
            "Checking 'anomalyNotes':\n",
            "  SUCCESS: Found 5 records.\n",
            "  Sample record(s):\n",
            "[\n",
            "  {\n",
            "    \"id\": 1,\n",
            "    \"text\": \"1. ExpressShip consistently exceeds expected delivery times by 7-10 days, particularly for Raw Material\\nC.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": 2,\n",
            "    \"text\": \"GlobalComp and FastSupply have quality scores below our acceptable threshold of 85%.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "Checking 'recommendations':\n",
            "  SUCCESS: Found 5 records.\n",
            "  Sample record(s):\n",
            "[\n",
            "  {\n",
            "    \"id\": 1,\n",
            "    \"text\": \"1. Conduct performance review with ExpressShip and consider alternative suppliers for Raw Material C.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": 2,\n",
            "    \"text\": \"Implement quality improvement programs with GlobalComp and FastSupply.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "--- Verification Summary ---\n",
            "Basic verification PASSED. Visually inspect samples.\n",
            "\n",
            "--- Running Graphviz Example ---\n",
            "ERROR during chunk indexing: name 'Digraph' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADD THIS VERIFICATION BLOCK ---\n",
        "# AFTER this line in your final execution block:\n",
        "# api_response_data = run_full_analysis_pipeline(system, query, pdf_path)\n",
        "\n",
        "print(\"\\n\\n--- Verifying structuredDataJSON Content ---\")\n",
        "\n",
        "if \"structuredDataJSON\" not in api_response_data:\n",
        "    print(\"ERROR: 'structuredDataJSON' key is missing from the results.\")\n",
        "else:\n",
        "    json_data = api_response_data[\"structuredDataJSON\"]\n",
        "    if not isinstance(json_data, dict):\n",
        "        print(f\"ERROR: 'structuredDataJSON' is not a dictionary, but type: {type(json_data)}\")\n",
        "    elif \"error\" in json_data:\n",
        "         print(f\"ERROR: Extraction failed: {json_data['error']}\")\n",
        "    else:\n",
        "        print(\"Found 'structuredDataJSON' dictionary. Checking contents...\")\n",
        "        expected_keys = [\n",
        "            \"deliveryPerformance\", \"supplierQuality\", \"carbonFootprint\",\n",
        "            \"networkConnectivity\", \"priceFluctuations\", \"inventoryLevels\",\n",
        "            \"anomalyNotes\", \"recommendations\"\n",
        "        ]\n",
        "        all_good = True\n",
        "\n",
        "        for key in expected_keys:\n",
        "            print(f\"\\nChecking '{key}':\")\n",
        "            if key not in json_data:\n",
        "                print(f\"  WARNING: Key '{key}' is missing from structuredDataJSON.\")\n",
        "                all_good = False\n",
        "            else:\n",
        "                data_list = json_data[key]\n",
        "                if not isinstance(data_list, list):\n",
        "                    print(f\"  ERROR: Value for '{key}' is not a list (type: {type(data_list)}).\")\n",
        "                    all_good = False\n",
        "                elif not data_list: # Check if the list is empty\n",
        "                    # Notes/Recs might be empty, others likely shouldn't be if extraction worked\n",
        "                    if key not in [\"anomalyNotes\", \"recommendations\"]:\n",
        "                         print(f\"  WARNING: List for '{key}' is empty. No rows parsed?\")\n",
        "                         # Consider if this is expected or an error for specific keys\n",
        "                         # all_good = False # You might set this if empty lists are unexpected\n",
        "                    else:\n",
        "                         print(f\"  INFO: List for '{key}' is empty (acceptable for notes/recs).\")\n",
        "\n",
        "                else:\n",
        "                    # If the list has data, print success and sample\n",
        "                    print(f\"  SUCCESS: Found {len(data_list)} records for '{key}'.\")\n",
        "                    print(f\"  Sample record(s) for '{key}':\")\n",
        "                    # Pretty print the first 2 records for inspection\n",
        "                    try:\n",
        "                        print(json.dumps(data_list[:2], indent=2))\n",
        "                        # Optional: Check keys of the first record\n",
        "                        if data_list[0] and isinstance(data_list[0], dict):\n",
        "                            print(f\"  Record keys: {list(data_list[0].keys())}\")\n",
        "                        else:\n",
        "                             print(\"  WARNING: First item is not a dictionary.\")\n",
        "                             all_good=False\n",
        "\n",
        "                    except Exception as print_err:\n",
        "                        print(f\"  Error printing sample data: {print_err}\")\n",
        "                        all_good = False\n",
        "\n",
        "        print(\"\\n--- Verification Summary ---\")\n",
        "        if all_good:\n",
        "            print(\"Basic verification PASSED. Most sections seem to have non-empty lists of data.\")\n",
        "            print(\"Please visually inspect the sample records above for correctness.\")\n",
        "        else:\n",
        "            print(\"Verification encountered WARNINGS or ERRORS. Review the messages above.\")\n",
        "\n",
        "# --- The rest of your script (Graphviz etc.) can follow ---"
      ],
      "metadata": {
        "id": "cH8p51jRSvBs",
        "outputId": "a077add3-b6c1-456d-c389-f4c1626f4872",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Verifying structuredDataJSON Content ---\n",
            "Found 'structuredDataJSON' dictionary. Checking contents...\n",
            "\n",
            "Checking 'deliveryPerformance':\n",
            "  WARNING: List for 'deliveryPerformance' is empty. No rows parsed?\n",
            "\n",
            "Checking 'supplierQuality':\n",
            "  WARNING: List for 'supplierQuality' is empty. No rows parsed?\n",
            "\n",
            "Checking 'carbonFootprint':\n",
            "  WARNING: List for 'carbonFootprint' is empty. No rows parsed?\n",
            "\n",
            "Checking 'networkConnectivity':\n",
            "  WARNING: List for 'networkConnectivity' is empty. No rows parsed?\n",
            "\n",
            "Checking 'priceFluctuations':\n",
            "  WARNING: List for 'priceFluctuations' is empty. No rows parsed?\n",
            "\n",
            "Checking 'inventoryLevels':\n",
            "  WARNING: List for 'inventoryLevels' is empty. No rows parsed?\n",
            "\n",
            "Checking 'anomalyNotes':\n",
            "  SUCCESS: Found 5 records for 'anomalyNotes'.\n",
            "  Sample record(s) for 'anomalyNotes':\n",
            "[\n",
            "  {\n",
            "    \"id\": 1,\n",
            "    \"text\": \"1. ExpressShip consistently exceeds expected delivery times by 7-10 days, particularly for Raw Material\\nC.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": 2,\n",
            "    \"text\": \"GlobalComp and FastSupply have quality scores below our acceptable threshold of 85%.\"\n",
            "  }\n",
            "]\n",
            "  Record keys: ['id', 'text']\n",
            "\n",
            "Checking 'recommendations':\n",
            "  SUCCESS: Found 5 records for 'recommendations'.\n",
            "  Sample record(s) for 'recommendations':\n",
            "[\n",
            "  {\n",
            "    \"id\": 1,\n",
            "    \"text\": \"1. Conduct performance review with ExpressShip and consider alternative suppliers for Raw Material C.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": 2,\n",
            "    \"text\": \"Implement quality improvement programs with GlobalComp and FastSupply.\"\n",
            "  }\n",
            "]\n",
            "  Record keys: ['id', 'text']\n",
            "\n",
            "--- Verification Summary ---\n",
            "Basic verification PASSED. Most sections seem to have non-empty lists of data.\n",
            "Please visually inspect the sample records above for correctness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def create_recommendation_map(root_issue, recommendations):\n",
        "    \"\"\"\n",
        "    Generates a mind map of recommendations stemming from a root issue.\n",
        "\n",
        "    Args:\n",
        "    - root_issue (str): The central problem statement.\n",
        "    - recommendations (list of str): List of recommendations related to the root issue.\n",
        "    \"\"\"\n",
        "    dot = Digraph(comment='Recommendation Map')\n",
        "    dot.attr(rankdir='LR')  # Layout left to right\n",
        "    dot.attr('node', shape='box', style='rounded,filled', color='lightblue2')\n",
        "\n",
        "    # Add root issue node\n",
        "    dot.node('root', root_issue, shape='ellipse', color='lightcoral')\n",
        "\n",
        "    # Add recommendation nodes\n",
        "    for i, rec in enumerate(recommendations):\n",
        "        node_id = f'rec_{i}'\n",
        "        dot.node(node_id, rec)\n",
        "        dot.edge('root', node_id)\n",
        "\n",
        "    # Render to file\n",
        "    filename = dot.render('recommendation_map', format='png', cleanup=True)\n",
        "    print(f\"Recommendation map saved to {filename}\")\n",
        "\n",
        "# Example usage\n",
        "root_issue = \"High customer churn\"\n",
        "recommendations = [\n",
        "    \"Improve onboarding experience\",\n",
        "    \"Implement loyalty program\",\n",
        "    \"Reduce response time of support team\",\n",
        "    \"Enhance product tutorial and FAQ\",\n",
        "    \"Gather exit feedback from churned users\"\n",
        "]\n",
        "\n",
        "create_recommendation_map(root_issue, recommendations)\n"
      ],
      "metadata": {
        "id": "X_s7Qk1h1u8V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}